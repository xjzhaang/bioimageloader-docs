{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0fc8ce2-8a64-441a-b093-34df81db5ceb",
   "metadata": {},
   "source": [
    "# DL: Training Stardist and Cellpose Models\n",
    "- Date: 2022-05-05\n",
    "- Author: Xingjian Zhang (https://github.com/xjzhaang)\n",
    "\n",
    "This notebook and the following **DL: Benchmark Table** is part of a small experiment.\n",
    "\n",
    "The aim is to build a benchmark table for `bioimageloader`'s collection of instance segmentation datasets, using built-in and custom trained models.\n",
    "\n",
    "In this notebook, we use [StarDist](https://github.com/stardist/stardist) and [Cellpose](https://github.com/mouseLand/cellpose) with `bioimageloader` to train our own models using a combined dataset.\n",
    "\n",
    "It serves as a demonstration on how you can use `bioimageloader` to do model training. The code blocks can be easily modified to adapt to your own tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcade3d-09b7-4c14-abcb-c3e2eae0e2f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. StarDist\n",
    "\n",
    "This tutorial is adapted from the github notebook (https://github.com/stardist/stardist/blob/master/examples/2D/2_training.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdb62dac-c428-4c95-bde0-c9fc16cc5257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n",
      "env: TF_CPP_MIN_LOG_LEVEL=3\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%env TF_CPP_MIN_LOG_LEVEL=3\n",
    "\n",
    "#Built-in \n",
    "import warnings\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "#Ignoring warnings for notebook compilation (might not work)\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "#Bioimageloader and Albumentation\n",
    "import albumentations as A\n",
    "from bioimageloader import Config, BatchDataloader, ConcatDataset\n",
    "from bioimageloader.transforms import SqueezeGrayImageHWC, HWCToCHW\n",
    "from bioimageloader.collections import (BBBC020, ComputationalPathology, S_BSST265, \n",
    "    DSB2018, FRUNet, BBBC039, BBBC006, Cellpose, LIVECell)\n",
    "\n",
    "#Stardist\n",
    "from stardist import fill_label_holes, random_label_cmap, calculate_extents\n",
    "from stardist.matching import matching, matching_dataset\n",
    "from stardist.models import Config2D, StarDist2D\n",
    "from csbdeep.utils import Path, normalize\n",
    "\n",
    "#Cellpose imports\n",
    "import torch\n",
    "from cellpose import models\n",
    "\n",
    "#Other imports\n",
    "#!pip install matplotlib seaborn pandas tqdm numpy\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fe85fb-dc13-4a71-af0c-83b8f34cf4bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Loading datasets\n",
    "First, we load our collection of datasets with instance masks together: DSB2018, ComputationalPathology, BBBC006, BBBC020, BBBC039, S_BSST265, FRUNet, Cellpose and LIVECell. \n",
    "\n",
    "As each dataset have different numbers of images, we perform data augmentation using `albumentations` library for smaller datasets. We invert the ComPath and LIVECell datasets so they look more like the rest.\n",
    "\n",
    "The datasets are then combined using `ConcatDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "424e073d-e867-4d6a-9d77-65b2ca3cedf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformations\n",
    "transforms = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    SqueezeGrayImageHWC()\n",
    "])\n",
    "\n",
    "transforms_compath = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.InvertImg(p=1.0),\n",
    "    A.OneOf([\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "    ], p=0.66),\n",
    "    A.OneOf([\n",
    "        A.RandomBrightnessContrast(p=0.2),\n",
    "        A.Rotate(p=0.5, limit=80),\n",
    "    ], p=0.66),\n",
    "    SqueezeGrayImageHWC()\n",
    "])\n",
    "\n",
    "transforms_livecell = A.Compose([\n",
    "    A.Resize(512, 512),\n",
    "    A.RandomCrop(256,256),\n",
    "    A.InvertImg(p=1.0),\n",
    "    SqueezeGrayImageHWC() \n",
    "])\n",
    "\n",
    "transforms_020 = A.Compose([\n",
    "    A.Resize(512, 512),\n",
    "    A.RandomCrop(256,256),\n",
    "    A.OneOf([\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "    ], p=0.66),\n",
    "    A.OneOf([\n",
    "        A.RandomBrightnessContrast(p=0.2),\n",
    "        A.Rotate(p=0.5, limit=80),\n",
    "    ], p=0.66),\n",
    "    SqueezeGrayImageHWC() \n",
    "])\n",
    "\n",
    "bbbc020 = BBBC020('./Data/bbbc/020', grayscale=True, image_ch=[\"nuclei\"],\n",
    "                  transforms=transforms_020, num_samples=80)\n",
    "comp = ComputationalPathology('./Data/ComputationalPathology', grayscale=True,\n",
    "                              transforms=transforms_compath, num_samples=80)\n",
    "dsb2018 = DSB2018('./Data/data-science-bowl-2018', grayscale=True, training=True,\n",
    "                  transforms=transforms, num_samples=80)\n",
    "sbss = S_BSST265('./Data/BioStudies',\n",
    "                 transforms=transforms, num_samples=80)\n",
    "frunet = FRUNet('./Data/FRU_processing',\n",
    "                transforms=transforms, num_samples=80)\n",
    "bbbc006 = BBBC006('./Data/bbbc/006', grayscale=True,\n",
    "                  transforms=transforms, num_samples=80)\n",
    "bbbc039 = BBBC039('./Data/bbbc/039',\n",
    "                  transforms=transforms, num_samples=80)\n",
    "cellpose = Cellpose('./Data/cellpose', grayscale=True,\n",
    "                    transforms=transforms, num_samples=80)\n",
    "livecell = LIVECell('./Data/LIVECell', save_tif=False, training=True,\n",
    "                    transforms=transforms_livecell, num_samples=80)\n",
    "\n",
    "dset = ConcatDataset([bbbc020, comp, dsb2018, sbss, frunet,\n",
    "                      bbbc006, bbbc039, cellpose, livecell])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6240ff-7441-473a-b2df-b5e090c81e39",
   "metadata": {},
   "source": [
    "### Normalization and train/val split\n",
    "We follow StarDist's sample notebook to perform normalization and train/val split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f34fc2f0-5ab4-414f-92bd-82783c32a38d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4a2acb3daeb46c7910fd7bbcea04fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/720 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = list()\n",
    "Y = list()\n",
    "for d in tqdm(dset):\n",
    "    X.append(d[\"image\"])\n",
    "    Y.append(d[\"mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45787fad-a255-46a0-a064-9f0f06ae203a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8959fef1897485b88957f5cd4ac44e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/720 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89f1117db2b9476ab02d67079d418633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/720 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- training:       612\n",
      "- validation:     108\n"
     ]
    }
   ],
   "source": [
    "n_channel = 1 if X[0].ndim == 2 else X[0].shape[-1]\n",
    "axis_norm = (0,1)   # normalize channels independently\n",
    "# axis_norm = (0,1,2) # normalize channels jointly\n",
    "if n_channel > 1:\n",
    "    print(\"Normalizing image channels %s.\" % ('jointly' if axis_norm is None or 2 in axis_norm else 'independently'))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "X = [normalize(x,1,99.8,axis=axis_norm) for x in tqdm(X)]\n",
    "Y = [fill_label_holes(y) for y in tqdm(Y)]\n",
    "\n",
    "\n",
    "assert len(dset) > 1, \"not enough training data\"\n",
    "rng = np.random.RandomState(42)\n",
    "ind = rng.permutation(len(dset))\n",
    "n_val = max(1, int(round(0.15 * len(ind))))\n",
    "ind_train, ind_val = ind[:-n_val], ind[-n_val:]\n",
    "X_val, Y_val = [X[i] for i in ind_val]  , [Y[i] for i in ind_val]\n",
    "X_trn, Y_trn = [X[i] for i in ind_train], [Y[i] for i in ind_train] \n",
    "print('- training:       %3d' % len(X_trn))\n",
    "print('- validation:     %3d' % len(X_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b7bb15-1395-427b-b69a-68a34bbaf03b",
   "metadata": {},
   "source": [
    "### Initialize a stardist model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3be5405-f411-4192-809c-e86a41d79bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default values: prob_thresh=0.5, nms_thresh=0.4.\n",
      "median object size:      [16. 13.]\n",
      "network field of view :  [94 93]\n"
     ]
    }
   ],
   "source": [
    "#Default parameters used in Stardist's own example notebook\n",
    "n_rays = 32\n",
    "grid = (2,2)\n",
    "conf = Config2D (\n",
    "    n_rays       = n_rays,\n",
    "    grid         = grid,\n",
    "    use_gpu      = True,\n",
    "    n_channel_in = 1,\n",
    ")\n",
    "\n",
    "#Specify the name and directory of the model\n",
    "model = StarDist2D(conf, name='stardist_model_1', basedir='stardist_models')\n",
    "\n",
    "\n",
    "median_size = calculate_extents(list(Y), np.median)\n",
    "fov = np.array(model._axes_tile_overlap('YX'))\n",
    "print(f\"median object size:      {median_size}\")\n",
    "print(f\"network field of view :  {fov}\")\n",
    "if any(median_size > fov):\n",
    "    print(\"WARNING: median object size larger than field of view of the neural network.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39c28b5-5131-43e7-a907-72a8943e532a",
   "metadata": {},
   "source": [
    "### Training and optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06484447-51e7-4115-b3a5-287ae79c5c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 9s 93ms/step - loss: 1.4590 - prob_loss: 0.3824 - dist_loss: 5.3832 - prob_kld: 0.2577 - dist_relevant_mae: 5.3827 - dist_relevant_mse: 72.6304 - dist_dist_iou_metric: 0.2540 - val_loss: 1.3347 - val_prob_loss: 0.3480 - val_dist_loss: 4.9334 - val_prob_kld: 0.2275 - val_dist_relevant_mae: 4.9328 - val_dist_relevant_mse: 63.9465 - val_dist_dist_iou_metric: 0.2826\n",
      "\n",
      "Loading network weights from 'weights_best.h5'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NMS threshold = 0.3:  75%|████▌ | 15/20 [00:56<00:18,  3.78s/it, 0.199 -> 0.020]\n",
      "NMS threshold = 0.4:  75%|████▌ | 15/20 [01:20<00:26,  5.37s/it, 0.199 -> 0.016]\n",
      "NMS threshold = 0.5:  75%|████▌ | 15/20 [01:24<00:28,  5.62s/it, 0.199 -> 0.015]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using optimized values: prob_thresh=0.198454, nms_thresh=0.3.\n",
      "Saving to 'thresholds.json'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prob': 0.19845443151963815, 'nms': 0.3}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We train then optimize the thresholds of a stardist model using the default parameters. \n",
    "#Epochs is set to 1 for demonstration\n",
    "model.train(X_trn, Y_trn, validation_data=(X_val, Y_val), epochs=1)\n",
    "model.optimize_thresholds(X_val, Y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807b8d37-22d3-4f54-9bad-d3f8ba76b303",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Cellpose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015997fd-4e20-4d94-b7d5-4a0164c329ed",
   "metadata": {},
   "source": [
    "### Loading datasets\n",
    "\n",
    "\n",
    "We follow the same procedures as StarDist, except here we use `HWCToCHW` (explained in the previous notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b94d208-8064-46e9-879d-72eaf307dec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformations\n",
    "transforms = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    HWCToCHW() \n",
    "])\n",
    "\n",
    "transforms_compath = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.InvertImg(p=1.0),\n",
    "    A.OneOf([\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "    ], p=0.66),\n",
    "    A.OneOf([\n",
    "        A.RandomBrightnessContrast(p=0.2),\n",
    "        A.Rotate(p=0.5, limit=80),\n",
    "    ], p=0.66),\n",
    "    HWCToCHW()\n",
    "])\n",
    "\n",
    "transforms_livecell = A.Compose([\n",
    "    A.Resize(512, 512),\n",
    "    A.RandomCrop(256,256),\n",
    "    A.InvertImg(p=1.0),\n",
    "    HWCToCHW()\n",
    "])\n",
    "\n",
    "transforms_020 = A.Compose([\n",
    "    A.Resize(512, 512),\n",
    "    A.RandomCrop(256,256),\n",
    "    A.OneOf([\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "    ], p=0.66),\n",
    "    A.OneOf([\n",
    "        A.RandomBrightnessContrast(p=0.2),\n",
    "        A.Rotate(p=0.5, limit=80),\n",
    "    ], p=0.66),\n",
    "    HWCToCHW()\n",
    "])\n",
    "\n",
    "bbbc020 = BBBC020('./Data/bbbc/020', grayscale=True, image_ch=[\"nuclei\"],\n",
    "                  transforms=transforms_020, num_samples=80)\n",
    "comp = ComputationalPathology('./Data/ComputationalPathology', grayscale=True,\n",
    "                              transforms=transforms_compath, num_samples=80)\n",
    "dsb2018 = DSB2018('./Data/data-science-bowl-2018', grayscale=True, training=True,\n",
    "                  transforms=transforms, num_samples=80)\n",
    "sbss = S_BSST265('./Data/BioStudies',\n",
    "                 transforms=transforms, num_samples=80)\n",
    "frunet = FRUNet('./Data/FRU_processing',\n",
    "                transforms=transforms, num_samples=80)\n",
    "bbbc006 = BBBC006('./Data/bbbc/006', grayscale=True,\n",
    "                  transforms=transforms, num_samples=80)\n",
    "bbbc039 = BBBC039('./Data/bbbc/039',\n",
    "                  transforms=transforms, num_samples=80)\n",
    "cellpose = Cellpose('./Data/cellpose', grayscale=True,\n",
    "                    transforms=transforms, num_samples=80)\n",
    "livecell = LIVECell('./Data/LIVECell', save_tif=False, training=True,\n",
    "                    transforms=transforms_livecell, num_samples=80)\n",
    "\n",
    "dset = ConcatDataset([bbbc020, comp, dsb2018, sbss, frunet,\n",
    "                      bbbc006, bbbc039, cellpose, livecell])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25bf590-139b-47b4-b981-2c2caf39946f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list()\n",
    "Y = list()\n",
    "for d in tqdm(dset):\n",
    "    X.append(d[\"image\"])\n",
    "    Y.append(d[\"mask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa9aefd-aa71-4804-8d55-df19e28cd935",
   "metadata": {},
   "source": [
    "### Train/val split\n",
    "For Cellpose, we do not need to normalize the data before training, it is done by specifying `normalize = True` in the training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b4feb6-6f0a-48cc-9962-678673d37f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(dset) > 1, \"not enough training data\"\n",
    "rng = np.random.RandomState(42)\n",
    "ind = rng.permutation(len(dset))\n",
    "n_val = max(1, int(round(0.15 * len(ind))))\n",
    "ind_train, ind_val = ind[:-n_val], ind[-n_val:]\n",
    "X_val, Y_val = [X[i] for i in ind_val]  , [Y[i] for i in ind_val]\n",
    "X_trn, Y_trn = [X[i] for i in ind_train], [Y[i] for i in ind_train] \n",
    "print('- training:       %3d' % len(X_trn))\n",
    "print('- validation:     %3d' % len(X_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39943a1e-5b57-4afa-b64d-abea1a3a005d",
   "metadata": {},
   "source": [
    "### Initialize model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf36e3a-2cc4-43fb-b4a0-a8c005130b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We set n_epochs = 1 for demonstration\n",
    "model = models.CellposeModel(pretrained_model=None, diam_mean=15, gpu=True)\n",
    "model.train(train_data=X_trn, train_labels=Y_trn, train_files=None,\n",
    "            test_data=X_val, test_labels=Y_val, test_files=None,\n",
    "            normalize = True, \n",
    "            channels = [0,0],\n",
    "            save_path='cellpose_models',\n",
    "            save_every=1,\n",
    "            learning_rate=0.01,\n",
    "            n_epochs=1,\n",
    "            momentum=0.9,\n",
    "            weight_decay=0.00001,\n",
    "            batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
